{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pythainlp\n",
      "  Downloading pythainlp-4.0.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting requests>=2.22.0 (from pythainlp)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.22.0->pythainlp)\n",
      "  Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.22.0->pythainlp)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.22.0->pythainlp)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.22.0->pythainlp)\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading pythainlp-4.0.2-py3-none-any.whl (13.4 MB)\n",
      "   ---------------------------------------- 0.0/13.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/13.4 MB 812.7 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.9/13.4 MB 7.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.2/13.4 MB 14.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 3.3/13.4 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.4/13.4 MB 17.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 5.4/13.4 MB 18.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 6.1/13.4 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.9/13.4 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.2/13.4 MB 18.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.4/13.4 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.5/13.4 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.6/13.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.7/13.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.4 MB 22.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.4/13.4 MB 21.1 MB/s eta 0:00:00\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests, pythainlp\n",
      "Successfully installed certifi-2023.7.22 charset-normalizer-3.3.2 idna-3.4 pythainlp-4.0.2 requests-2.31.0 urllib3-2.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.5 MB 487.6 kB/s eta 0:00:04\n",
      "     -------- ------------------------------- 0.3/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 9.6 MB/s eta 0:00:00\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tewwa\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 regex-2023.10.3 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp.augment import WordNetAug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tewwa\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = WordNetAug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = aug.augment(\"เราชอบไปโรงเรียน\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ชอบ'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เขมือบ', 'เจี๊ยะ', 'ฉัน', 'ซัด', 'โซ้ย', 'แดก', 'บริโภค', 'ฟาด', 'โภค', 'ยัด', 'รับประทาน', 'สวาปาม', 'เสพ', 'เสวย', 'หม่ำ', 'แอ้ม']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "data={}\n",
    "temp=[]\n",
    "i=0\n",
    "with open('data.csv','r',encoding=\"utf-8-sig\") as csv_file:\n",
    "\tcsv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\tnext(csv_reader, None)\n",
    "\tfor row in csv_reader:\n",
    "\t\tfor d in [i for i in row[-1].split(\"|\") if i!=\"\"]:\n",
    "\t\t\tfor t in [i for i in row[-1].split(\"|\") if i!=\"\" and i!=d]:\n",
    "\t\t\t\tkey={}\n",
    "\t\t\t\tkey['word']=d\n",
    "\t\t\t\tkey['synonym']=t\n",
    "\t\t\t\ttemp.append(key)\n",
    "\t\t\tkey={}\n",
    "\t\t\tkey['word']=d\n",
    "\t\t\tkey['synonym']=row[0]\n",
    "\t\t\ttemp.append(key)\n",
    "\t\t\tkey={}\n",
    "\t\t\tkey['word']=row[0]\n",
    "\t\t\tkey['synonym']=d\n",
    "\t\t\ttemp.append(key)\n",
    "data[\"data\"]=temp\n",
    "def search(word):\n",
    "\treturn [a['synonym'] for a in data['data'] if a['word']==word]\n",
    "\n",
    "text=input(\"Text : \")\n",
    "get=search(text)\n",
    "if get!=[]:\n",
    "    print(get)\n",
    "else:\n",
    "    print(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['เขมือบ', 'เจี๊ยะ', 'ฉัน', 'ซัด', 'โซ้ย', 'แดก', 'บริโภค', 'ฟาด', 'โภค', 'ยัด', 'รับประทาน', 'สวาปาม', 'เสพ', 'เสวย', 'หม่ำ', 'แอ้ม']\n",
      "ฉันเขมือบข้าว\n",
      "ฉันเจี๊ยะข้าว\n",
      "ฉันฉันข้าว\n",
      "ฉันซัดข้าว\n",
      "ฉันโซ้ยข้าว\n",
      "ฉันแดกข้าว\n",
      "ฉันบริโภคข้าว\n",
      "ฉันฟาดข้าว\n",
      "ฉันโภคข้าว\n",
      "ฉันยัดข้าว\n",
      "ฉันรับประทานข้าว\n",
      "ฉันสวาปามข้าว\n",
      "ฉันเสพข้าว\n",
      "ฉันเสวยข้าว\n",
      "ฉันหม่ำข้าว\n",
      "ฉันแอ้มข้าว\n"
     ]
    }
   ],
   "source": [
    "h = search(\"กิน\")\n",
    "if h!=[]:\n",
    "    print(get)\n",
    "else:\n",
    "    print(\"not found\")\n",
    "    \n",
    "for i in range(len(h)):\n",
    "    print(f\"ฉัน{h[i]}ข้าว\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('เรา', 'ชอบ', 'ไป', 'ร.ร.'),\n",
       " ('เรา', 'ชอบ', 'ไป', 'รร.'),\n",
       " ('เรา', 'ชอบ', 'ไป', 'อาคารเรียน'),\n",
       " ('เรา', 'ชอบ', 'ไป', 'โรงเรียน'),\n",
       " ('เรา', 'ชอบ', 'ไปยัง', 'ร.ร.'),\n",
       " ('เรา', 'ชอบ', 'ไปยัง', 'รร.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], ['ครรไล', 'คระไล', 'ไคล', 'จร', 'เต้า'], [], [], [], ['ครรไล', 'คระไล', 'ไคล', 'จร', 'เต้า'], [], [], [], ['ครรไล', 'คระไล', 'ไคล', 'จร', 'เต้า'], [], [], [], ['ครรไล', 'คระไล', 'ไคล', 'จร', 'เต้า'], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(len(ls)):\n",
    "    for j in range(len(ls[i])):\n",
    "        res.append(search(ls[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ฉัน\n",
      "กินข้าว\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"Enter a sentence: \")\n",
    "tokens = word_tokenize(sentence)\n",
    "synonyms = []\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
